{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Objective:**  \n",
    "Develop a classification model to identify high-risk patients in the ICU and assign a triage tag. \n",
    "\n",
    "**Data**  \n",
    "Dataset was developed as part of MIT's GOSSIS community initiative, with privacy certification from the Harvard Privacy Lab. https://www.kaggle.com/c/widsdatathon2020/data\n",
    "\n",
    "**Triage Protocal**\n",
    "* Green: (wait) are reserved for the \"walking wounded\" who will need medical care at some point, after more critical injuries have been treated.\n",
    "* Yellow: (observation) for those who require observation (and possible later re-triage). Their condition is stable for the moment and, they are not in immediate danger of death. These victims will still need hospital care and would be treated immediately under normal circumstances.\n",
    "* Red: (immediate) are used to label those who cannot survive without immediate treatment but who have a chance of survival.\n",
    "* Black: (expectant) are used for the deceased and for those whose injuries are so extensive that they will not be able to survive given the care that is available.\n",
    "Note: this model will not identify black tags. Within the red group, physicians should assign additional tagging if needed. \n",
    "\n",
    "**Metrics**  \n",
    "The goal is to flag anyone entering the ICU who is at risk. With that, recall is the main metric although AUC-ROC and precision will be used as secondary considerations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load Data & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from load_data import access_aws, load_and_info, import_files, basic_data_info\n",
    "from eda import create_cat_df, pairplotting, selecting_cols, exploring_y\n",
    "from data_cleaning import clean_data\n",
    "from model_dev import (tts, test_model_framework, forced_grid_search, \n",
    "                        make_confusion_matrix, final_model)\n",
    "import assigning_triage_levels\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, \n",
    "                              AdaBoostClassifier, BaggingRegressor)\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from ipywidgets import interactive, FloatSlider\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Pull the full icu_mortality data from AWS for EDA and feature engineering\n",
    "query_all_data = '''SELECT * FROM icu_mortality_table'''\n",
    "\n",
    "#replace host IP each time instance is restarted\n",
    "aws_access = (aws_code())\n",
    "raw_data = access_aws('52.14.254.66', 'icu_mortality',query_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# call function to view basic information about the data\n",
    "basic_data_info(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Import Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Pull data_dict from AWS to help understand the raw_data file\n",
    "query_data_dict = '''SELECT * FROM icu_data_dict_table'''\n",
    "data_dict = access_aws('52.14.254.66', 'icu_mortality',query_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dict.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To view dataset in smaller batches, break the full set into smaller groups based on the \n",
    "# category that the column falls under occording to the data dictionary\n",
    "\n",
    "demo_data = create_cat_df(raw_data, 'demographic', data_dict, demo=True)\n",
    "apache_covariate_data = create_cat_df(raw_data, 'APACHE covariate', data_dict)\n",
    "vitals_data = create_cat_df(raw_data, 'vitals', data_dict)\n",
    "labs_data = create_cat_df(raw_data, 'labs', data_dict)\n",
    "labs_blood_gas_data = create_cat_df(raw_data, 'labs blood gas', data_dict)\n",
    "apache_prediction_data = create_cat_df(raw_data, 'APACHE prediction', data_dict)\n",
    "apache_comorbidity_data = create_cat_df(raw_data, 'APACHE comorbidity', data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Observe Dependent Variable - Hospital Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def exploring_y(data):\n",
    "    \"\"\"Reveals a few key characterstics of the y_value, hospital_death.\"\"\"\n",
    "    print(\"Unique hospital death values:\\n\", data.hospital_death.unique())\n",
    "    print(\"\\nNumber of each y value:\\n\", data.hospital_death.value_counts())\n",
    "    print(\"\\nPercentage of survivors:\\n\", data.hospital_death.value_counts(normalize=True)*100)\n",
    "    return\n",
    "\n",
    "exploring_y(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Observe Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def selecting_cols(data):\n",
    "    \"\"\"Takes in a dataframe and calculates the difference in mean between\n",
    "    those that survive and do not survive for each column. Returns a dataframe\n",
    "    of only columns with signfificant differentiation between the means as well\n",
    "    as a list of the columns included in the Dataframe. Differentiation is\n",
    "    considered signficant if mean(1)/mean(0) > 1.5 or <.75.\"\"\"\n",
    "\n",
    "    selected_cols = []\n",
    "    dif_in_means = data.groupby('hospital_death').mean().iloc[0,:] / data.groupby('hospital_death').mean().iloc[1,:]\n",
    "    high_end = dif_in_means.index[dif_in_means > 1.5].to_list()\n",
    "    low_end = dif_in_means.index[dif_in_means < .75].to_list()\n",
    "    selected_cols += high_end\n",
    "    selected_cols += low_end\n",
    "    selected_cols.append('hospital_death')\n",
    "    if len(selected_cols) > 1:\n",
    "        selected_data = data[selected_cols]\n",
    "        return selected_data, selected_cols\n",
    "    else:\n",
    "        print(\"No signficant features. Additional exploration needed.\")\n",
    "        return dif_in_means, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 7-Figure Summary Per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "demo_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "apache_covariate_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vitals_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labs_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labs_blood_gas_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "apache_prediction_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "apache_comorbidity_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Determinging Significant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to determing which features to begin building the model with, view the difference in values for each classification. If the difference in value for y = 1 and y = 0 is > 1.5 or < .75, feature will be considered significant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def selecting_cols(data):\n",
    "    \"\"\"Takes in a dataframe and calculates the difference in mean between\n",
    "    those that survive and do not survive for each column. Returns a dataframe\n",
    "    of only columns with signfificant differentiation between the means as well\n",
    "    as a list of the columns included in the Dataframe. Differentiation is\n",
    "    considered signficant if mean(1)/mean(0) > 1.5 or <.75.\"\"\"\n",
    "\n",
    "    selected_cols = []\n",
    "    dif_in_means = data.groupby('hospital_death').mean().iloc[0,:] / data.groupby('hospital_death').mean().iloc[1,:]\n",
    "    high_end = dif_in_means.index[dif_in_means > 1.5].to_list()\n",
    "    low_end = dif_in_means.index[dif_in_means < .75].to_list()\n",
    "    selected_cols += high_end\n",
    "    selected_cols += low_end\n",
    "    selected_cols.append('hospital_death')\n",
    "    if len(selected_cols) > 1:\n",
    "        selected_data = data[selected_cols]\n",
    "        return selected_data, selected_cols\n",
    "    else:\n",
    "        print(\"No signficant features. Additional exploration needed.\")\n",
    "        return dif_in_means, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "demo_df, demo_df_cols = selecting_cols(demo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "apach_cov_df, apach_cov_df_cols = selecting_cols(apache_covariate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vitals_df, vitals_df_cols = selecting_cols(vitals_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labs_df, labs_df_cols = selecting_cols(labs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labs_bg_df, labs_bg_df_cols = selecting_cols(labs_blood_gas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "apach_pred_df, apach_pred_df_cols = selecting_cols(apache_prediction_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "apach_com_df, apach_com_df_cols = selecting_cols(apache_comorbidity_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drop_null(data):\n",
    "    \"\"\"Drops any columns with > 50% of the columns missing. From remaining columsn,\n",
    "    drops all rows with null values.\"\"\"\n",
    "\n",
    "    cols_with_many_null = (data.isna().sum() / len(data)).sort_values(ascending = False)\n",
    "    cols_with_many_null = cols_with_many_null.index[cols_with_many_null > 0.50]\n",
    "    print('There are %d columns with more than 50%% missing values' % len(cols_with_many_null))\n",
    "\n",
    "    #drop columns with > 50% null values\n",
    "    data = data.drop(cols_with_many_null, axis=1)\n",
    "    print(\"Columns dropped\")\n",
    "\n",
    "    data = data.dropna()\n",
    "    print(\"Rows with nulls dropped.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def clean_data(data):\n",
    "    #remove values of icu days, hospital death prob, and icu death prob that are less than 0\n",
    "    #as these values are not possible\n",
    "    data1 = data[data.pre_icu_los_days >= 0]\n",
    "    data2 = data[data.apache_4a_hospital_death_prob >= 0]\n",
    "    data3 = data[data.apache_4a_icu_death_prob >= 0]\n",
    "\n",
    "    #fill in columns with many nulls to indicate weather test was performed or not\n",
    "    data3.h1_lactate_max = data3.h1_lactate_max.fillna(0)\n",
    "    data3.h1_lactate_max = data3.h1_lactate_max.apply(lambda x: 1 if x != 0 else x)\n",
    "    data3.h1_inr_max = data3.h1_inr_max.fillna(0)\n",
    "    data3.h1_inr_max = data3.h1_inr_max.apply(lambda x: 1 if x != 0 else x)\n",
    "    data3.bilirubin_apache = data3.bilirubin_apache.fillna(0)\n",
    "    data3.bilirubin_apache = data3.bilirubin_apache.apply(lambda x: 1 if x != 0 else x)\n",
    "\n",
    "    #replace nulls in hospital_admit_source with 'unkown'\n",
    "    data3.hospital_admit_source = data3.hospital_admit_source.fillna('unkown')\n",
    "\n",
    "    #remove additional nulls from dataset\n",
    "    cleaned_data = drop_null(data3)\n",
    "\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cleaned_data = clean_data(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cleaned_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Creating data groups by category with clean data\n",
    "demo_data_clean = create_cat_df(cleaned_data, 'demographic', data_dict, demo=True)\n",
    "apache_covariate_data_clean = create_cat_df(cleaned_data, 'APACHE covariate', data_dict)\n",
    "vitals_data_clean = create_cat_df(cleaned_data, 'vitals', data_dict)\n",
    "labs_data_clean = create_cat_df(cleaned_data, 'labs', data_dict)\n",
    "labs_blood_gas_data_clean = create_cat_df(cleaned_data, 'labs blood gas', data_dict)\n",
    "apache_prediction_data_clean = create_cat_df(cleaned_data, 'APACHE prediction', data_dict)\n",
    "apache_comorbidity_data_clean = create_cat_df(cleaned_data, 'APACHE comorbidity', data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#reselecting columns based on cleaned data for significant categories\n",
    "demo_df, demo_df_cols = selecting_cols(demo_data_clean)\n",
    "apach_cov_df, apach_cov_df_cols = selecting_cols(apache_covariate_data_clean)\n",
    "labs_df, labs_df_cols = selecting_cols(labs_data_clean)\n",
    "apach_pred_df, apach_pred_df_cols = selecting_cols(apache_prediction_data_clean)\n",
    "apach_com_df, apach_com_df_cols = selecting_cols(apache_comorbidity_data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In a separate notebook, combinations of the columns selected above were tested in logistic regression and random forest models. Through that process, the columns and engineered features below were selected as top performers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Selecting columns based on above EDA\n",
    "features = apach_pred_df_cols + demo_df_cols + apach_cov_df_cols + cat_cols_to_keep\n",
    "\n",
    "# In above category separation, hospital_death, the dependent variable, is included in all of the \n",
    "# categories. It therefore is removed twice to ensure only one copy of hospital_death is in the \n",
    "# dataset\n",
    "features.remove('hospital_death')\n",
    "features.remove('hospital_death')\n",
    "\n",
    "# create new dataframe with these final features\n",
    "model_df = cleaned_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Additional feature engineering to increase signal and decrease noise in weaker features\n",
    "model_df.pre_icu_los_days = model_df.pre_icu_los_days.apply(lambda x: x**2)\n",
    "model_df['bun_creatinine'] = model_df.bun_apache * combo5_df.creatinine_apache\n",
    "model_df = model_df.drop(['bun_apache', 'creatinine_apache'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save cleaned, engineered dataframe for later use\n",
    "model_df.to_csv('model_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In a separate notebook, a full exploration of classification models including Decision Trees, Logistic Regression, K-Nearest Neighbors (KNN), and Support Vector Machines was conducted. Random Forests performed best with the highest recall, auc, and precision scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dummy_data(data):\n",
    "    \"\"\"Creates dummy variables for each categorical column.\"\"\"\n",
    "\n",
    "    cat_cols = [col for col in data.columns if (data[col].dtype == 'O')]\n",
    "    dummied_data = pd.get_dummies(data, drop_first=True, columns=cat_cols)\n",
    "    print(\"Dummy variables created.\")\n",
    "    return dummied_data\n",
    "\n",
    "def create_x_y(data):\n",
    "    \"\"\"Creates X and y variables where y = 'hospital_death' column and\n",
    "    X = remaining columns.\"\"\"\n",
    "\n",
    "    X, y = data.drop('hospital_death', axis=1), data['hospital_death']\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tts(data):\n",
    "    \"\"\"Splits the dataset into train and test sets. Calls create_x_y to\n",
    "    split X and y, Calls dummy_data to turn any categorical columns into\n",
    "    dummy variables.\"\"\"\n",
    "    dummied_data = dummy_data(data)\n",
    "\n",
    "    X, y = create_x_y(dummied_data)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, stratify = y)\n",
    "    print(\"Dataset split into train and test sets.\\n\")\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_train shape:\", y_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_tuning, X_test_tuning, y_train_tuning, y_test_tuning = tts(model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def roc_curve_plotting(fpr, tpr, auc_roc):\n",
    "    \"\"\"Takes in fpr, tpr, and auc_roc to produce an ROC curve for all\n",
    "    validation sets.\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    lw = 2\n",
    "    for i in range(len(fpr)):\n",
    "        plt.plot(fpr[i], tpr[i],\n",
    "                 lw=lw, label='ROC curve (area = %0.2f)' % auc_roc[i])\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def scaling_data(X_tr, X_val):\n",
    "    \"\"\"Separates train and validation datasets into categorical and numerical columns.\n",
    "    Scales numerical columns using StandardScaler\"\"\"\n",
    "\n",
    "    #separating numerical columns in train and validation sets\n",
    "    num_cols = [col for col in X_tr.columns if (X_tr[col].dtype == float)]\n",
    "    X_tr_num, X_tr_not_num = X_tr[num_cols], X_tr.drop(num_cols, axis=1)\n",
    "    X_val_num, X_val_not_num = X_val[num_cols], X_val.drop(num_cols, axis=1)\n",
    "\n",
    "    #scaling numerical columns only\n",
    "    ss = StandardScaler()\n",
    "    X_tr_scaled = ss.fit_transform(X_tr_num)\n",
    "    X_val_scaled = ss.transform(X_val_num)\n",
    "\n",
    "    #rejoining scaled columns with non-scaled columns\n",
    "    X_tr = np.concatenate((X_tr_scaled, X_tr_not_num), axis=1)\n",
    "    X_val = np.concatenate((X_val_scaled, X_val_not_num), axis=1)\n",
    "\n",
    "    return X_tr, X_val\n",
    "\n",
    "# generic model framework used to test a variety of models with a consistent function\n",
    "\n",
    "def test_model_framework(X_train, y_train, model, scaling=False, smote=False, rand=False):\n",
    "    \"\"\"Within the 80% train data, splits the model into 5 folds for cross-validation testing.\n",
    "    If model being implemented requires scaling, allows scaling function to be called.\n",
    "    Also enables smote or random oversampling to be called.\"\"\"\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 71)\n",
    "    print(\"Kfolds created.\")\n",
    "\n",
    "    recall, precision, accuracy, matrix, roc_auc = [], [], [], [], []\n",
    "    fpr, tpr = [], []\n",
    "    p, t, pr_thresholds = [], [], []\n",
    "\n",
    "    # all scaling and oversampling must be performed within the k-fold cross validation to avoid\n",
    "    # data leakage\n",
    "    for train_ind, val_ind in kf.split(X_train,y_train):\n",
    "\n",
    "        X_tr, y_tr = X_train.iloc[train_ind], y_train.iloc[train_ind]\n",
    "        X_val, y_val = X_train.iloc[val_ind], y_train.iloc[val_ind]\n",
    "\n",
    "        #standard scaling\n",
    "        if scaling:\n",
    "            X_tr, X_val = scaling_data(X_tr, X_val)\n",
    "\n",
    "        if smote:\n",
    "            ros = SMOTE(random_state=19)\n",
    "            X_tr, y_tr = ros.fit_sample(X_tr, y_tr)\n",
    "\n",
    "        if rand:\n",
    "            ros = RandomOverSampler(random_state=19)\n",
    "            X_tr, y_tr = ros.fit_sample(X_tr, y_tr)\n",
    "\n",
    "        mod = model\n",
    "        mod.fit(X_tr, y_tr)\n",
    "        prediction_prob = mod.predict_proba(X_val)\n",
    "        prediction = mod.predict(X_val)\n",
    "        \n",
    "        # for each k-fold, append recall, precision, accuracy, and confusion matrix scores to\n",
    "        # lists above\n",
    "        recall.append(recall_score(y_val, prediction, average='binary', pos_label=1))\n",
    "        precision.append(precision_score(y_val, prediction, average='binary', pos_label=1))\n",
    "        accuracy.append(accuracy_score(y_val, prediction))\n",
    "        matrix.append(confusion_matrix(y_val, prediction))\n",
    "\n",
    "        # For each k-fold, calculate roc_curve parameters and append to lists above \n",
    "        fpr_ex, tpr_ex, thresholds_ex = roc_curve(y_val, prediction_prob[:,1], pos_label=1)\n",
    "        fpr.append(fpr_ex)\n",
    "        tpr.append(tpr_ex)\n",
    "        roc_auc.append(roc_auc_score(y_val, prediction_prob[:,1]))\n",
    "\n",
    "    # After cross validation, print out the mean scores\n",
    "    print(\"Confusion matrix:\\n\", (matrix[0] + matrix[1] + matrix[2] + matrix[3] + matrix[4])/5)\n",
    "    print(\"Mean recall: \", np.mean(recall))\n",
    "    print(\"Mean precision: \", np.mean(precision))\n",
    "    print(\"Mean accuracy: \", np.mean(accuracy))\n",
    "\n",
    "    # plot the mean roc curve\n",
    "    roc_curve_plotting(fpr, tpr, roc_auc)\n",
    "\n",
    "    # returns the model so it can be used later on for any plotting\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def forced_grid_search(X_train, y_train):\n",
    "    \"\"\"Mimics grid search but calls test_model_framework function to include oversampling within\n",
    "    the k-fold.\"\"\"\n",
    "    for n in [100,200,500]:\n",
    "        for m in [5,10,50,100,500]:\n",
    "            for z in [5,10,50,100,500]:\n",
    "                print(f\"For n_estimators = {n}, max_depth = {m}, min_samples_leaf={z}\")\n",
    "                rf_tuning = RandomForestClassifier(n_estimators=n,\n",
    "                                                max_depth=m,\n",
    "                                                min_samples_leaf=10)\n",
    "\n",
    "                tuning_model_run = test_model_framework(X_train, y_train, rf_tuning,\n",
    "                                                scaling=False, smote=False, rand=True)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test a variety of different random forest conditions to tune model\n",
    "forced_grid_search(X_train_tuning, y_train_tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Soft voting classifier with logistic regression, random forest, and knn\n",
    "model_list = [('rf', RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=2,min_samples_leaf=10)), \n",
    "              ('lr', LogisticRegression(solver='liblinear', C=.01)),\n",
    "              ('knn', KNeighborsClassifier(n_neighbors=100))]\n",
    "\n",
    "voting_classifer = feature_testing_simple_model(X_train_fe, y_train_fe, VotingClassifier(estimators=model_list, \n",
    "                                                                      voting='soft',\n",
    "                                                                      n_jobs=-1), \n",
    "                             scaling=True, smote=False, rand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# stacked classifier with same 3 models and logistic regression metaclassifier\n",
    "\n",
    "models = [RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=2,min_samples_leaf=10), \n",
    "            LogisticRegression(solver='liblinear', C=.01),\n",
    "            KNeighborsClassifier(n_neighbors=100)]\n",
    "\n",
    "stacked = StackingClassifier(classifiers=models, \n",
    "                             meta_classifier=LogisticRegression(C=.01), \n",
    "                             use_probas=True, use_features_in_secondary=False)\n",
    "\n",
    "stacking_classifier = feature_testing_simple_model(X_train_fe, y_train_fe, stacked, scaling=True, smote=False, rand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# stacked classifier with random forest and logistic regression. \n",
    "# Random forest metaclassifier with reusing features in secondary model run. \n",
    "\n",
    "models = [RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=2,min_samples_leaf=10), \n",
    "            LogisticRegression(solver='liblinear', C=.01)]\n",
    "\n",
    "stacked = StackingClassifier(classifiers=models, \n",
    "                             meta_classifier=RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=2,min_samples_leaf=10), \n",
    "                             use_probas=True, use_features_in_secondary=True)\n",
    "\n",
    "stacking_classifier = feature_testing_simple_model(X_train, y_train, stacked, scaling=True, smote=False, rand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Threshold Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use best performing parameters to create a final model\n",
    "\n",
    "def final_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Takes in train test split values, trains a random forest model with best-performing \n",
    "    hyperparameters and prints out model metrics.\"\"\"\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=19)\n",
    "    X_train, y_train = ros.fit_sample(X_train, y_train)\n",
    "\n",
    "    # best performing metrics during testing\n",
    "    model = RandomForestClassifier(n_estimators = 100,\n",
    "                                   max_depth = 5,\n",
    "                                   min_samples_leaf=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = prediction = model.predict(X_test)\n",
    "\n",
    "    print(\"Model Recall: \", recall_score(y_test, prediction, average='binary', pos_label=1))\n",
    "    print(\"Model Precision: \", precision_score(y_test, prediction))\n",
    "    print(\"Model Accuracy: \", accuracy_score(y_test, prediction))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create interactive threshold adjuster to observe changes in the confusion matrix\n",
    "\n",
    "def make_confusion_matrix(X_test, y_test, model, threshold):\n",
    "    # Predict class 1 if probability of being in class 1 is greater than threshold\n",
    "    # (model.predict(X_test) does this automatically with a threshold of 0.5)\n",
    "    y_predict = (model.predict_proba(X_test)[:, 1] >= threshold)\n",
    "    icu_confusion = confusion_matrix(y_test, y_predict)\n",
    "    plt.figure(dpi=80)\n",
    "    sns.heatmap(icu_confusion, cmap=plt.cm.Blues, annot=True, square=True, fmt='d',\n",
    "           xticklabels=['Surviving', 'Not-surviving'],\n",
    "           yticklabels=['Surviving', 'Not-surviving']);\n",
    "    plt.xlabel('prediction')\n",
    "    plt.ylabel('actual')\n",
    "    recall = icu_confusion[1][1] / (icu_confusion[1][1] + icu_confusion[1][0])\n",
    "    precision = icu_confusion[1][1] / (icu_confusion[1][1] + icu_confusion[0][1])\n",
    "    print('Recall: ', recall)\n",
    "    print('Precision: ', precision)\n",
    "    th_prec = pd.DataFrame({'threshold': threshold, 'prediction': y_predict})\n",
    "\n",
    "    return\n",
    "\n",
    "interactive(lambda threshold: make_confusion_matrix(X_test, y_test, final_model, threshold), threshold=(0.0,1.0,0.02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Append Prediction Column To Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to create a Tableu dashboard that shows triage levels in the ICU for patient administration and physicians, append a predictions column back to the origonal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    if row['flag'] <= .3:\n",
    "        val = 'G'\n",
    "    elif .3 < row['flag'] <= .7:\n",
    "        val = 'Y'\n",
    "    elif .7 < row['flag'] <= 1:\n",
    "        val = 'R'\n",
    "    return val\n",
    "\n",
    "def add_flags_to_original_dataset(model, X_test, original_data):\n",
    "    \"\"\"Adds flags back to the original dataset without feature engineered\n",
    "    columns for better visualization.\"\"\"\n",
    "    X_test_indices = X_test.index\n",
    "    X_test_indices = [ind for ind in X_test_indices]\n",
    "    X_test_from_original = original_data.loc[X_test_indices,:]\n",
    "    X_test_from_original_flagged = pd.concat((X_test_from_original.reset_index(), pred_proba_df), axis=1)\n",
    "    X_test_from_original_flagged = X_test_from_original_flagged.rename(columns={'index':'patient_id'})\n",
    "\n",
    "    X_test_from_original_flagged['flag'] = X_test_from_original_flagged.pred_proba\n",
    "    X_test_from_original_flagged['flag'] = X_test_from_original_flagged.apply(f, axis=1)\n",
    "\n",
    "    X_test_proba_appended.to_csv('original_data_with_flag.csv')\n",
    "\n",
    "    return X_test_proba_appended\n",
    "\n",
    "original_data_with_flags = create_flag_column(final_model, X_test, raw_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
